{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit8ebcdbbb0f8548a88014d2138d5dd4f5",
   "display_name": "Python 3.7.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "89550c41512397be3b92a88a74e3b1e8549c4151dd6ce085be64a8e2e8a2e248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "this was just a little sandbox, ignore!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('https://www.xwordinfo.com/Crossword?date=1/10/2000')\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'38'"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "def get_stats():\n",
    "    '''returns a dict containing rows, columns, words, blocks, and missing letters'''\n",
    "    stat_block = {}\n",
    "    stats = soup.find_all('div',{'id':'CPHContent_StatsData'})[0].find_all('span')\n",
    "    stat_block['rows'] = re.search('(?<=Rows: )\\d+',stats[0].get_text()).group(0)\n",
    "    stat_block['columns'] = re.search('(?<=Columns: )\\d+',stats[0].get_text()).group(0)\n",
    "    stat_block['words'] = re.search('(?<=Words: )\\d+',stats[1].get_text()).group(0)\n",
    "    stat_block['blocks'] = re.search('(?<=Blocks: )\\d+',stats[1].get_text()).group(0)\n",
    "    stat_block['missing'] = re.search('({)(.+)(})',stats[4].get_text()).group(2)\n",
    "    return stat_block\n",
    "\n",
    "stats_dict = get_stats()\n",
    "\n",
    "stats_dict['blocks']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'New York Times, Monday, January 10, 2000'"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "puz_title = soup.find_all('h1', {'id' : 'PuzTitle'})[0].get_text()\n",
    "puz_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\"Live free or die,\" to New Hampshire'"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "\n",
    "puz_clue = soup.find_all('h2', {'class' : 'keyclue'})[0].get_text()\n",
    "puz_clue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Will Shortz'"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "\n",
    "def get_authors():\n",
    "    '''returns a dict containing puzzle author and editor'''\n",
    "    author_info = soup.find_all('div', {'id' : 'CPHContent_AEGrid'})[0]\n",
    "    scraped_info = {}\n",
    "    for index, div in enumerate(author_info):\n",
    "        try:\n",
    "            if div.get_text() == 'Author:':\n",
    "                scraped_info['author'] = list(author_info)[index+1].get_text()\n",
    "            elif div.get_text() == 'Editor:':\n",
    "                scraped_info['editor'] = list(author_info)[index+1].get_text()\n",
    "        except:\n",
    "            continue\n",
    "    return scraped_info\n",
    "\n",
    "get_authors()['editor']"
   ]
  },
  {
   "source": [
    "TODO: add the above as columns in the csv creation function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}